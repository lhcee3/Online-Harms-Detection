{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lhcee3/Online-Harms-Detection/blob/main/Online_harms_detection_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQ09ggoxiHpl"
      },
      "source": [
        "Importing Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBPxtjf-aBD-",
        "outputId": "d0af126d-a7fe-44cd-b263-372e8833a55b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.11/dist-packages (0.3.6)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from kagglehub) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kagglehub) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kagglehub) (4.67.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2025.1.31)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install kagglehub nltk scikit-learn pandas\n",
        "\n",
        "import kagglehub\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixu_MXLvaDeH"
      },
      "source": [
        "Download & Load Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jl3vvzy4aBf6",
        "outputId": "c2f85f1b-84f3-4f09-a9c7-86103e349576"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.6), please consider upgrading to the latest version (0.3.8).\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/mrmorj/hate-speech-and-offensive-language-dataset?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.01M/1.01M [00:00<00:00, 53.3MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.6), please consider upgrading to the latest version (0.3.8).\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/tushifire/ldnoobw?dataset_version_number=2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 23.6k/23.6k [00:00<00:00, 23.6MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n",
            "Total bad words collected: 2612\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Download Hate Speech Dataset\n",
        "path1 = kagglehub.dataset_download(\"mrmorj/hate-speech-and-offensive-language-dataset\")\n",
        "df1 = pd.read_csv(f\"{path1}/labeled_data.csv\")\n",
        "\n",
        "# Download LDNOOBW Dataset\n",
        "path2 = kagglehub.dataset_download(\"tushifire/ldnoobw\")\n",
        "\n",
        "\n",
        "df2_files = os.listdir(path2)\n",
        "df2_files = [file for file in df2_files if file not in [\"LICENSE\", \"README.md\", \"USERS.md\"]]\n",
        "\n",
        "\n",
        "bad_words = set()\n",
        "for file in df2_files:\n",
        "    file_path = os.path.join(path2, file)\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        bad_words.update([line.strip().lower() for line in f])\n",
        "\n",
        "print(\"Total bad words collected:\", len(bad_words))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8gTRpkzaG2r"
      },
      "source": [
        "Preprocess Hate Speech Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJq3SoIzaFBG"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "import pandas as pd\n",
        "import re\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Text preprocessing function using spaCy\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
        "    text = re.sub(r'\\W', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "\n",
        "    doc = nlp(text)\n",
        "\n",
        "\n",
        "    tokens = [token.lemma_ for token in doc if token.text not in STOP_WORDS and not token.is_punct]\n",
        "\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "\n",
        "df1['processed_text'] = df1['tweet'].apply(preprocess_text)\n",
        "df1['label'] = df1['class']\n",
        "\n",
        "\n",
        "X = df1['processed_text']\n",
        "y = df1['label']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gn_u6dygchRc"
      },
      "source": [
        "SVM Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xhxv_Qktaiw6",
        "outputId": "a0bba095-4c9d-4f60-86e9-ed465882e1a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimized Model Accuracy: 0.9305\n",
            "\n",
            "Optimized Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.96      0.93      3849\n",
            "           1       0.98      0.84      0.91      3794\n",
            "           2       0.92      0.99      0.95      3871\n",
            "\n",
            "    accuracy                           0.93     11514\n",
            "   macro avg       0.93      0.93      0.93     11514\n",
            "weighted avg       0.93      0.93      0.93     11514\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['optimized_tfidf_vectorizer.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Improved TF-IDF Vectorization\n",
        "vectorizer = TfidfVectorizer(\n",
        "    max_features=10000,\n",
        "    ngram_range=(1, 2),\n",
        "    min_df=5\n",
        ")\n",
        "X_tfidf = vectorizer.fit_transform(X)\n",
        "\n",
        "\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X_tfidf, y)\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
        "\n",
        "# Hyperparameter tuning using GridSearchCV\n",
        "param_grid = {\"C\": [0.01, 0.1, 1, 10, 100]}\n",
        "grid_search = GridSearchCV(LinearSVC(class_weight=\"balanced\"), param_grid, cv=5, scoring=\"accuracy\")\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "best_svm = grid_search.best_estimator_\n",
        "y_pred = best_svm.predict(X_test)\n",
        "\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Optimized Model Accuracy: {accuracy:.4f}\")\n",
        "print(\"\\nOptimized Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# Saving the model\n",
        "import joblib\n",
        "joblib.dump(best_svm, \"optimized_svm_model.pkl\")\n",
        "joblib.dump(vectorizer, \"optimized_tfidf_vectorizer.pkl\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "\n",
        "joblib.dump(best_svm, \"optimized_svm_model.pkl\")\n",
        "\n",
        "\n",
        "joblib.dump(vectorizer, \"optimized_tfidf_vectorizer.pkl\")\n",
        "\n",
        "print(\"Model and vectorizer saved successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SS0dqhWdTlfF",
        "outputId": "9ca5c821-bec2-4620-f693-af1347116e64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model and vectorizer saved successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.listdir())  # Should show \"['.config', 'optimized_tfidf_vectorizer.pkl', 'optimized_svm_model.pkl', 'sample_data']\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOteQkr1UQKJ",
        "outputId": "7fcc61b2-8ceb-4ffd-dad5-0abba775a47c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['.config', 'optimized_svm_model.pkl', 'optimized_tfidf_vectorizer.pkl', 'sample_data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To Try The Model\n"
      ],
      "metadata": {
        "id": "niC--pvATPDO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jeuXNQ7ZceOP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9eb6a924-fbe8-46db-9583-4113e3b33204"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Enter a statement (or type 'exit' to stop): he is bad guy\n",
            "DEBUG: Raw Prediction Output: 1\n",
            "Classification: Offensive Language\n",
            "\n",
            "Enter a statement (or type 'exit' to stop): exit\n",
            "Exiting...\n"
          ]
        }
      ],
      "source": [
        "import joblib\n",
        "import spacy\n",
        "import re\n",
        "\n",
        "\n",
        "svm_model = joblib.load(\"optimized_svm_model.pkl\")\n",
        "tfidf_vectorizer = joblib.load(\"optimized_tfidf_vectorizer.pkl\")\n",
        "\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "\n",
        "def is_question(text):\n",
        "    return text.strip().endswith(\"?\")\n",
        "\n",
        "\n",
        "def preprocess_text(text):\n",
        "    doc = nlp(text.lower())\n",
        "    tokens = [token.lemma_ for token in doc if token.is_alpha]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "\n",
        "def classify_text(text):\n",
        "    if is_question(text):\n",
        "        return \"Neutral/Acceptable (Detected as a Question)\"\n",
        "\n",
        "    processed_text = preprocess_text(text)\n",
        "    text_vectorized = tfidf_vectorizer.transform([processed_text])\n",
        "    prediction = svm_model.predict(text_vectorized)[0]\n",
        "\n",
        "    print(f\"DEBUG: Raw Prediction Output: {prediction}\")\n",
        "\n",
        "\n",
        "    label_mapping = {\n",
        "        0: \"Hate Speech\",\n",
        "        1: \"Offensive Language\",\n",
        "        2: \"Neutral/Acceptable\"\n",
        "    }\n",
        "\n",
        "    return label_mapping.get(prediction, \"Unknown\")\n",
        "\n",
        "# Interactive Input in Google Colab\n",
        "def interactive_test():\n",
        "    while True:\n",
        "        user_input = input(\"\\nEnter a statement (or type 'exit' to stop): \")\n",
        "        if user_input.lower() == \"exit\":\n",
        "            print(\"Exiting...\")\n",
        "            break\n",
        "        result = classify_text(user_input)\n",
        "        print(f\"Classification: {result}\")\n",
        "\n",
        "\n",
        "interactive_test()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u2uSRY9tq52P"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMOx+cUWAIcRrYUiQH/QLrQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}